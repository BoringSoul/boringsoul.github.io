<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="author" content="andrew chung" /><link rel="canonical" href="https://boringsoul.github.io/middleware/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Kafka/" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>深入理解Kafka：核心设计与实践原理 - 安崽的笔记</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "\u6df1\u5165\u7406\u89e3Kafka\uff1a\u6838\u5fc3\u8bbe\u8ba1\u4e0e\u5b9e\u8df5\u539f\u7406";
        var mkdocs_page_input_path = "middleware\\\u6df1\u5165\u7406\u89e3Kafka.md";
        var mkdocs_page_url = "/middleware/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Kafka/";
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/java.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/python.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> 安崽的笔记
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="" href="../../../">简介</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">消息队列</span></p>
              <ul class="current">
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">深入理解Kafka：核心设计与实践原理</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#kafka">第一章 初识Kafka</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#11">1.1 基本概念</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#12">1.2 安装和配置</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#13">1.3 生产和消费</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#14">1.4 服务端参数配置</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#_1">第二章：生产者</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#21">2.1 客户端开发</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#211">2.1.1 拦截器</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#212">2.1.2 序列化器</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#213">2.1.3 分区器</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#214">2.1.4 消息发送</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#22">2.2 原理分析</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#221">2.2.1 整体架构</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#222">2.2.2 元数据更新</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#23">2.3 重要的生产者参数</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#_2">第三章 消费者</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#31">3.1 消费者与消费组</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#32">3.2 客户端开发</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#321">3.2.1 必要的参数配置</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#322_kafkaconsumer">3.2.2 KafkaConsumer 初始化源码</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#324">3.2.4 订阅主题与分区</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#325">3.2.5 消费者拦截器</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#326">3.2.6 反序列化</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    </ul>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">数据库相关</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../database/%E9%AB%98%E6%80%A7%E8%83%BDMySQL/">高性能MySQL-第三版</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">安崽的笔记</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">消息队列</li>
      <li class="breadcrumb-item active">深入理解Kafka：核心设计与实践原理</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h2 id="-kafka">笔记-深入理解Kafka：核心设计与实践原理<a class="headerlink" href="#-kafka" title="Permanent link">&para;</a></h2>
<p>简单记下笔记</p>
<h3 id="kafka">第一章 初识Kafka<a class="headerlink" href="#kafka" title="Permanent link">&para;</a></h3>
<p>首先知道他是啥：</p>
<ul>
<li>消息系统： 就是一个消息队列系统，作用无非就那几个-解耦，削峰、异步，然后再加上自己的集群特性，做到可扩展、可恢复</li>
<li>持久化： log文件存消息</li>
<li>流式处理平台：提供接口来做窗口、连接、变换和聚合等等的操作</li>
</ul>
<h4 id="11">1.1 基本概念<a class="headerlink" href="#11" title="Permanent link">&para;</a></h4>
<blockquote>
<p>看下图就好：</p>
</blockquote>
<p><img alt="Kafka 基本架构图" src="../assets/img/blog16949bd6279df106.png" /></p>
<ul>
<li><em>Producer</em> : 生产者</li>
<li><em>Broker</em> : 就是 <em>Kafka Server</em></li>
<li><em>Consumer</em> : 消费者</li>
<li><em>Message.Topic</em>: 消息是按Topic去分的</li>
<li><em>Message.Topic.Partition</em> : <em>Topic</em> 划分为不同的分区存储，分区的数量、分配规则，能通过配置文件进行配置，同时还能在运行时，动态修改创建主题时的分区数量（只能增，不能减）、分配规则</li>
<li><em>Message.Topic.Partition.Replicas</em>: 分区时有副本的，每个分区的副本数量，分配规则也是跟上面一样，可以静态设置，动态修改</li>
<li><em>AR( Assigned Replicas )</em>, <em>ISR (In-Sync Replicas)</em>, <em>OSR (Out-Sync Replicas)</em> =&gt; <em>AR</em> = <em>ISR</em> + <em>OSR</em></li>
<li>"Leader Replicas": 主副本， 负责读写</li>
<li>"Follower Replica": 从副本，负责记录猪副本数据，有一定滞后性</li>
<li><em>Message.Topic.Partition.Replica.LEO</em>: 该分区下一条消息待写入的 <em>Offset</em></li>
<li><em>Message.Topic.Partition.Replica.HW</em>: <em>HW - 1</em> = 消费者能消费到的最大 <em>Offset</em></li>
<li><em>Message.Topic.Partition.Log</em> : 在持久化层上可以把分区看成一个追加的 <em>Log</em> 文件</li>
<li><em>Message.Topic.Partition.Offset</em> : 指消息在分区中的一个偏移量，且是分区中的唯一标识，也就是 <em>Kafka</em> 是分区内有序的</li>
</ul>
<h4 id="12">1.2 安装和配置<a class="headerlink" href="#12" title="Permanent link">&para;</a></h4>
<blockquote>
<p>3.x 版本安装比较简单，<em>windows/linux</em> 版本都是直接下个包，在本地跑脚本就好了</p>
</blockquote>
<h4 id="13">1.3 生产和消费<a class="headerlink" href="#13" title="Permanent link">&para;</a></h4>
<blockquote>
<p>这部分代码在后面的章节是会贴出来了，这里就不再赘述了</p>
</blockquote>
<h4 id="14">1.4 服务端参数配置<a class="headerlink" href="#14" title="Permanent link">&para;</a></h4>
<blockquote>
<p>这里贴出最常见的几个参数说明一下，配置的内容都是在 <em>config/server.properties</em> 里面，3.x 版本的话，如果要用 <em>kraft</em> 的话，则看 <em>config/kraft/server.properties</em> 文件</p>
</blockquote>
<ul>
<li><em>process.roles</em> : 进程的角色列表，逗号分割，默认：<em>broker,controller</em></li>
<li><em>node.id</em> : 节点 <em>id</em>，用来标识节点，在日志可以用来排查问题</li>
<li><em>listeners</em> : <em>broker</em> 监听客户端的协议+地址+端口,格式为 : 协议://地址:端口，协议值为：<em>PLAINTEXT, SSL, SASL_SSL</em></li>
<li><em>log.dirs</em> : 持久化目录，<em>log.dirs</em> 优先级大于 <em>log.dir</em></li>
</ul>
<h3 id="_1">第二章：生产者<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h3>
<h4 id="21">2.1 客户端开发<a class="headerlink" href="#21" title="Permanent link">&para;</a></h4>
<blockquote>
<p>这里我先贴整体的代码，然后再细说</p>
</blockquote>
<pre><code class="language-Java">package org.example;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerRecord;

import java.util.Properties;

public class KafkaProducerDemo {

    public static final String brokerList = &quot;localhost:19092,localhost:29092,localhost:39092&quot;;
    public static final String topic = &quot;test&quot;;


    public static Properties initConfig() {
        Properties props = new Properties();
        props.put(&quot;bootstrap.servers&quot;, brokerList);
        props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);
        props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);
        props.put(&quot;client.id&quot;, &quot;producer.client.id.demo&quot;);
        return props;
    }

    public static void main(String[] args) {
        Properties props = initConfig();
        try (KafkaProducer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props)) {
            ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(topic, &quot;hello kafka&quot;);
            producer.send(record);
        } catch (Exception e) {
            System.out.println(e.getMessage());
        }

    }
}
</code></pre>
<h5 id="211">2.1.1 拦截器<a class="headerlink" href="#211" title="Permanent link">&para;</a></h5>
<p><em>ProducerInterceptor</em> 接口是这样的</p>
<pre><code class="language-java">public interface ProducerInterceptor&lt;K, V&gt; extends Configurable, AutoCloseable {
    ProducerRecord&lt;K, V&gt; onSend(ProducerRecord&lt;K, V&gt; var1);

    void onAcknowledgement(RecordMetadata var1, Exception var2);

    void close();
}
</code></pre>
<blockquote>
<p>拦截器的注入方式是这样的，源码用反射<em>(getDeclaredConstructor().newInstance())</em>去实例化对象</p>
</blockquote>
<pre><code class="language-java">props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, KafkaProducerInterceptor.class.getName());
</code></pre>
<p>以 kafka-client 3.7.0 为例</p>
<p><em>onSend</em> 方法是在 <em>KafkaProducer</em> 源码 357 行执行的</p>
<pre><code class="language-java">public Future&lt;RecordMetadata&gt; send(ProducerRecord&lt;K, V&gt; record, Callback callback) {
    ProducerRecord&lt;K, V&gt; interceptedRecord = this.interceptors.onSend(record);
    return this.doSend(interceptedRecord, callback);
}
</code></pre>
<p><em>onAcknoledgement</em> 则是在 703 行触发，就是在 <em>onComplete</em> 的时候调用</p>
<pre><code class="language-java">public void onCompletion(RecordMetadata metadata, Exception exception) {
    if (metadata == null) {
        metadata = new RecordMetadata(this.topicPartition(), -1L, -1, -1L, -1, -1);
    }

    this.interceptors.onAcknowledgement(metadata, exception);
    if (this.userCallback != null) {
        this.userCallback.onCompletion(metadata, exception);
    }

}
</code></pre>
<h5 id="212">2.1.2 序列化器<a class="headerlink" href="#212" title="Permanent link">&para;</a></h5>
<p><em>StringSerializer</em> 源码如下</p>
<pre><code class="language-java">package org.apache.kafka.common.serialization;

import java.io.UnsupportedEncodingException;
import java.nio.charset.StandardCharsets;
import java.util.Map;
import org.apache.kafka.common.errors.SerializationException;

public class StringSerializer implements Serializer&lt;String&gt; {
    private String encoding;

    public StringSerializer() {
        this.encoding = StandardCharsets.UTF_8.name();
    }

    public void configure(Map&lt;String, ?&gt; configs, boolean isKey) {
        String propertyName = isKey ? &quot;key.serializer.encoding&quot; : &quot;value.serializer.encoding&quot;;
        Object encodingValue = configs.get(propertyName);
        if (encodingValue == null) {
            encodingValue = configs.get(&quot;serializer.encoding&quot;);
        }

        if (encodingValue instanceof String) {
            this.encoding = (String)encodingValue;
        }

    }

    public byte[] serialize(String topic, String data) {
        try {
            return data == null ? null : data.getBytes(this.encoding);
        } catch (UnsupportedEncodingException var4) {
            throw new SerializationException(&quot;Error when serializing string to byte[] due to unsupported encoding &quot; + this.encoding);
        }
    }
}
</code></pre>
<p>没啥特别，就是 <em>serialize</em> 里返回了 <em>string.getBytes()</em>
自定义的也是一样，返回 <em>byte[]</em> 就好，反序列化道理一样</p>
<h5 id="213">2.1.3 分区器<a class="headerlink" href="#213" title="Permanent link">&para;</a></h5>
<p>先看看源码定义</p>
<pre><code class="language-java">package org.apache.kafka.clients.producer;

import java.io.Closeable;
import org.apache.kafka.common.Cluster;
import org.apache.kafka.common.Configurable;

public interface Partitioner extends Configurable, Closeable {
    int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster);

    void close();

    /** @deprecated */
    @Deprecated
    default void onNewBatch(String topic, Cluster cluster, int prevPartition) {
    }
}
</code></pre>
<p>默认分区器 <em>DefaultPartioner</em> 源码是这样的</p>
<pre><code class="language-java">public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster, int numPartitions) {
    return keyBytes == null ? this.stickyPartitionCache.partition(topic, cluster) : BuiltInPartitioner.partitionForKey(keyBytes, numPartitions);
}
</code></pre>
<p>主要看key是否为空，如果空的话调用 <em>stickyPartitionCache.partition</em>， 随机一个可用的分区来写入 <p>
非空就用内置分区器，先用 <em>Utils.murmur2</em> 来 <em>hash</em>， 最后 % <em>numPartitions</em></p>
<h5 id="214">2.1.4 消息发送<a class="headerlink" href="#214" title="Permanent link">&para;</a></h5>
<p>发送模式有三种: 发后即忘(<em>fire-and-forget</em>)、 同步 (<em>sync</em>)、 异步(<em>async</em>)，其实 <em>send</em> 返回的是 <em>Future</em> 对象，所以传参肯定有 <em>Callback</em> 的入参，要同步模式的话，直接 <em>Future.get</em> 即可。
这里讲讲增加可靠性的套路</p>
<blockquote>
<p>注意： 提高可靠性会导致性能下降</p>
</blockquote>
<pre><code class="language-java">props.put(ProducerConfig.RETRIES_CONFIG, 10); // 增加重试次数
props.put(ProducerConfig.ACKS_CONFIG, -1); //  确保所有副本有响应了才当成功
</code></pre>
<h4 id="22">2.2 原理分析<a class="headerlink" href="#22" title="Permanent link">&para;</a></h4>
<h5 id="221">2.2.1 整体架构<a class="headerlink" href="#221" title="Permanent link">&para;</a></h5>
<blockquote>
<p>完整的发送流程图如下:</p>
</blockquote>
<p><img alt="Kafka 生产者发送流程图" src="../assets/img/%E7%94%9F%E4%BA%A7%E8%80%85%E5%8F%91%E9%80%81%E6%B5%81%E7%A8%8B.png" /></p>
<blockquote>
<p>整体流程分两个线程完成，一个主线程，一个发送线程, 1-5 主线程完成 </p>
</blockquote>
<ol>
<li>主线程通过 <em>KafkaProducer</em> 创建消息</li>
<li>看客户端有没有实现 <em>ProducerIntercepter</em> 这个接口，如果有就对消息进行拦截处理，一般拦截是为了对消息做简单处理，因为太复杂会影响发送速度，同时这玩意是可以链式编程的，里面的 <em>onSend()</em> 在发送前触发, <em>onAcknowleagement()</em> 在 <em>Selector</em> 返回 <em>Response</em> 后马上触发？</li>
<li>对消息的 <em>key</em>, <em>value</em> 进行序列化操作，要自定义，就实现 <em>org.apache.kafka.common.serlization.Seralizer</em> 接口， <em>configure()</em> 配置当前类，
<em>serialize(String topic, T data)</em> 就是序列化方法，返回 <em>byte[]</em>, <em>close()</em> 用来关闭序列化器</li>
<li>知道 <em>Topic</em> 了，就得知道消息往哪个分区发了，如果没有自定义分区器，那么就用默认方法来确定发送的分区，如果有就根据自定义的结果确定分区号</li>
<li>主线程把消息缓存到对应的 <em>RecordAccumulator</em> 中， 数据结构是 <em>Map&lt;Partiion, Deque\<ProducerBatch\>></em> ，<em>buffer.memory</em> 控制大小，消息太多会阻塞发送方法， <em>max.block.ms</em> 控制阻塞等待时间，<em>batch.size</em> 控制 <em>ProducerBatch</em> 大小</li>
<li><em>Sender</em> 线程创建 <em>Sender</em> 对象，把 <em>RecordAccumulator</em>.<em>Map&lt;Partiion, Deque\<ProducerBatch\>></em> 转换成 <em>Map&lt;Node, List\<ProducerBatch\>></em>，最后封成 <em><Node, Request></em> 对象</li>
<li><em><Node, Request></em> 转成 <em>Map&lt;NodeId, Deque\<Request\>></em> 放到 <em>InFlightRequests</em> 中, 这玩意是用来记录发了，但没有响应的 <em>Request</em>， <em>max.in.flight.requests.per.connection</em> 默认为5，表示每个连接最大等待响应的请求数是5，超过5就不能发送了。还能根据 <em>Deque<Request>.size()</em> 来判断每个 <em>Node</em> 的负载情况</li>
</ol>
<h5 id="222">2.2.2 元数据更新<a class="headerlink" href="#222" title="Permanent link">&para;</a></h5>
<blockquote>
<p>元数据指的是各个节点的基本信息，包括节点地址、节点主题、节点的分区、分区 <em>leader/follower</em> 副本信息，<em>AR/ISR</em>, <em>Controller</em> 节点</p>
</blockquote>
<p>上述的元数据是通过 <em>Sender</em> 线程来更新的，更新步骤如下
1. 找出 <em>leastLoadedNode</em> (<em>count</em> 一下 <em>InFlightRequests</em>.<em>Deque<Request>.size()</em>)
2. 发送 <em>MetadataRequest</em>，然后缓存到 <em>InFlightRequests</em></p>
<h4 id="23">2.3 重要的生产者参数<a class="headerlink" href="#23" title="Permanent link">&para;</a></h4>
<blockquote>
<p>这些参数都是在创建 <em>Producer</em> 的时候放到 <em>props</em> 里面的</p>
</blockquote>
<ul>
<li><em>ack</em>: <p> 
        = 1 表示 <em>leader</em> 副本成功写入，就会收到来自服务端的写入成功响应，数据有可能会丢 <p>
        = 0 表示不用等服务端任何响应。速度最快是它了, 但它最容易丢数据 <p>
        =-1 表示所有 <em>ISR</em> 成功写入才返回成功, 最大可靠性是它了</li>
<li><em>max.request.size</em>: 客户端能发送消息的最大值，默认 1048576<em>B</em>=1<em>M</em>，必须 <em>&lt;= broker</em>.<em>message.max.size</em></li>
<li><em>retries</em>: 生产者重试次数</li>
<li><em>retry.backoff.ms</em> : 重试之间的间隔，默认 100</li>
<li><em>compression.type</em>: 消息压缩方式，默认 <em>none</em>，可以为 <em>gzip</em> <em>snappy</em> <em>lz4</em></li>
<li><em>connections.max.idle.ms</em>: 每个限制连接最大存活时间</li>
<li><em>linger.ms</em>: 发送 <em>ProducerBacth</em> 之前等待更多消息的时间，用来控制吞吐量的</li>
<li><em>recieve.buffer.byte</em>: 控制 <em>Socket</em> 接收缓冲区的大小, 默认值是 32768(<em>B</em>)=32<em>KB</em></li>
<li><em>send.buffer.byte</em>: 控制 <em>Socket</em> 发送缓冲区的大小, 默认值是 131072(<em>B</em>)=128<em>KB</em></li>
</ul>
<h3 id="_2">第三章 消费者<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h3>
<h4 id="31">3.1 消费者与消费组<a class="headerlink" href="#31" title="Permanent link">&para;</a></h4>
<blockquote>
<p>看下图就知道大概意思了</p>
</blockquote>
<p><img alt="消费者与消费组消费某个Topic的消息" src="../assets/img/3.1.png" /></p>
<p>消费者与消费组、分区的关系就以下几点：</p>
<ol>
<li>消费者是以分区为单位进行消费</li>
<li>同一 <em>Topic</em> 下的消费组，里面的消费者会均摊 <em>Topic</em> 内的 <em>partition</em>，所以可能会出现 <em>partition</em> 不够分的情况</li>
<li>消费组内的消费者，在消费同一 <em>topic</em> 下的消息，显然只会消费一次，相当于 <em>p2p</em> 模式</li>
<li>如果是不同组的消费者，就相当于广播模式了</li>
</ol>
<h4 id="32">3.2 客户端开发<a class="headerlink" href="#32" title="Permanent link">&para;</a></h4>
<blockquote>
<p>老规矩，先把抄的代码贴出来</p>
</blockquote>
<pre><code class="language-java">package org.example.consumer;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;

import java.time.Duration;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.concurrent.atomic.AtomicBoolean;

public class KafkaConsumerDemo {

    public static final String BROKER_LIST = &quot;localhost:19092,localhost:29092,localhost:39092&quot;;
    public static final String TOPIC = &quot;test&quot;;

    private static final String GROUP_ID = &quot;group.demo&quot;;

    public static final AtomicBoolean IS_RUNNING = new AtomicBoolean(true);

    public static Map&lt;String, Object&gt; initConfig() {
        Map&lt;String, Object&gt; props = new HashMap&lt;&gt;();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, BROKER_LIST);
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);
        props.put(ConsumerConfig.CLIENT_ID_CONFIG, &quot;producer.client.id.demo&quot;);
        props.put(ConsumerConfig.GROUP_ID_CONFIG, GROUP_ID);
        return props;
    }

    public static void main(String[] args) {
        try (KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(initConfig())) {
            consumer.subscribe(List.of(TOPIC));
            while (IS_RUNNING.get()) {
                ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(1000));
                for (ConsumerRecord&lt;String, String&gt; record : records) {
                    System.out.printf(&quot; TOPIC = %s, partition = %s, offset = %s, key = %s, value = %s \n&quot;,
                            record.topic(), record.partition(), record.offset(), record.key(), record.value());
                }
            }
        } catch (Exception e) {
            System.out.println(e.getMessage());
        }
    }
}
</code></pre>
<h5 id="321">3.2.1 必要的参数配置<a class="headerlink" href="#321" title="Permanent link">&para;</a></h5>
<blockquote>
<p>一般都是那几个了，有了生产者代码的经验之后，瞄一下就好了，还有就是生产上最好用 <em>ConsumerConfig</em> 的枚举，自己写容易出错</p>
</blockquote>
<ul>
<li><em>bootstrap.server</em>: 连的 <em>kafka.brokers</em> 的列表</li>
<li><em>group.id</em>: 就是消费者组 <em>id</em>, 如果是空的， 会提示 <em>To use the group management or offset commit APIs, you must provide a valid group.id in the consumer configuration.</em> ( <em>kafka-client v 3.7.0</em> )</li>
<li><em>key.deserializer</em>: <em>key</em> 的反序列化器， 跟生产者的 <em>key.serializer</em> 反过来就行</li>
<li><em>value.deserializer</em>: <em>value</em> 的反序列化器， 跟生产者的 <em>value.serializer</em> 反过来就行</li>
</ul>
<h5 id="322_kafkaconsumer">3.2.2 <em>KafkaConsumer</em> 初始化源码<a class="headerlink" href="#322_kafkaconsumer" title="Permanent link">&para;</a></h5>
<blockquote>
<p>这部分干货，原书没有的，我是在找拦截器触发时机的时候看源码，觉得挺有意思的，捞出来给大家分享</p>
</blockquote>
<p><em>KafkaConsumer</em> 这个类其实是用了代理模式生成了实际执行的 <em>KafkaConsumer</em> 对象</p>
<pre><code class="language-java">public class KafkaConsumer&lt;K, V&gt; implements Consumer&lt;K, V&gt;{
    KafkaConsumer(ConsumerConfig config, Deserializer&lt;K&gt; keyDeserializer, Deserializer&lt;V&gt; valueDeserializer) {
        this.delegate = CREATOR.create(config, keyDeserializer, valueDeserializer);
    }

    KafkaConsumer(LogContext logContext, Time time, ConsumerConfig config, Deserializer&lt;K&gt; keyDeserializer, Deserializer&lt;V&gt; valueDeserializer, KafkaClient client, SubscriptionState subscriptions, ConsumerMetadata metadata, List&lt;ConsumerPartitionAssignor&gt; assignors) {
        this.delegate = CREATOR.create(logContext, time, config, keyDeserializer, valueDeserializer, client, subscriptions, metadata, assignors);
    }
}
</code></pre>
<blockquote>
<p>里面根据 <em>group.protocol</em> 生成两种消费者对象，一个是 <em>AsyncKafkaConsumer</em>, 一个是 <em>legacyKafkaConsumer</em></p>
</blockquote>
<pre><code class="language-java">public &lt;K, V&gt; ConsumerDelegate&lt;K, V&gt; create(ConsumerConfig config, Deserializer&lt;K&gt; keyDeserializer, Deserializer&lt;V&gt; valueDeserializer) {
        try {
            GroupProtocol groupProtocol = GroupProtocol.valueOf(config.getString(&quot;group.protocol&quot;).toUpperCase(Locale.ROOT));
            return (ConsumerDelegate)(groupProtocol == GroupProtocol.CONSUMER ? new AsyncKafkaConsumer(config, keyDeserializer, valueDeserializer) : new LegacyKafkaConsumer(config, keyDeserializer, valueDeserializer));
        } catch (KafkaException var5) {
            throw var5;
        } catch (Throwable var6) {
            throw new KafkaException(&quot;Failed to construct Kafka consumer&quot;, var6);
        }
    }
</code></pre>
<h5 id="324">3.2.4 订阅主题与分区<a class="headerlink" href="#324" title="Permanent link">&para;</a></h5>
<blockquote>
<p>这一个章节主要讲的是 <em>subsribe()</em> 和 <em>assign()</em> 方法</p>
</blockquote>
<p>主题订阅支持的模式:</p>
<ol>
<li><em>AUTO_TOPICS</em>: 全名称匹配，但是需要传主题数组做为传参, </li>
<li><em>AUTO_PATTERN</em>: 正则匹配模式</li>
<li><em>USER_ASSIGNED</em>: 用户分配模式，分配指定 <em>topic</em> 下的 <em>partition</em></li>
</ol>
<p>主题订阅的时候可以传入再平衡的回调方法</p>
<h5 id="325">3.2.5 消费者拦截器<a class="headerlink" href="#325" title="Permanent link">&para;</a></h5>
<blockquote>
<p>跟原书顺序不一样，我是按照代码执行顺序来讲的</p>
<blockquote>
<p>拦截器的作用其实跟其他框架都一样，都是给目标对象一个切面，在触发某些事件/动作的前后插入一些自定义的方法，先贴一下 <em>ConsumerInterceptor</em> 的接口源码</p>
</blockquote>
</blockquote>
<pre><code class="language-java">public interface ConsumerInterceptor&lt;K, V&gt; extends Configurable, AutoCloseable {
    ConsumerRecords&lt;K, V&gt; onConsume(ConsumerRecords&lt;K, V&gt; var1);

    void onCommit(Map&lt;TopicPartition, OffsetAndMetadata&gt; var1);

    void close();
}
</code></pre>
<blockquote>
<p><em>OnConsume</em> 方法触发时机在 <em>AsyncKafkaConsumer.poll()/LegacyKafakConsumer.poll()</em> 中，当执行了 <em>pollForFethces</em> 之后会返回 <em>fetch</em> 接着就执行 <em>onConsume(fetch.records())</em></p>
</blockquote>
<p>例子</p>
<pre><code class="language-java">package org.example.consumer;

import org.apache.kafka.clients.consumer.ConsumerInterceptor;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.common.TopicPartition;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

public class KafkaConsumerInterceptor implements ConsumerInterceptor&lt;String, String&gt; {

    private static final long EXPIRE_INTERVAL = 10 * 1000;

    @Override
    public ConsumerRecords&lt;String, String&gt; onConsume(ConsumerRecords&lt;String, String&gt; records) {
        long now = System.currentTimeMillis();
        Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;String, String&gt;&gt;&gt; newRecords = new HashMap&lt;&gt;();
        for (TopicPartition tp : records.partitions()) {
            List&lt;ConsumerRecord&lt;String, String&gt;&gt; list = records.records(tp);
            List&lt;ConsumerRecord&lt;String, String&gt;&gt; filter = new ArrayList&lt;&gt;();
            for (ConsumerRecord&lt;String, String&gt; i : list) {
                if(now - i.timestamp() &lt;  EXPIRE_INTERVAL){
                    filter.add(i);
                }
            }
            newRecords.put(tp, filter);
        }
        return new ConsumerRecords&lt;&gt;(newRecords);
    }

    @Override
    public void close() {

    }

    @Override
    public void onCommit(Map map) {

    }

    /**
     * configure不是直接继承自Interceptor的
     *
     * @param map 配置map
     */
    @Override
    public void configure(Map&lt;String, ?&gt; map) {

    }
}
</code></pre>
<h5 id="326">3.2.6 反序列化<a class="headerlink" href="#326" title="Permanent link">&para;</a></h5>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../../database/%E9%AB%98%E6%80%A7%E8%83%BDMySQL/" class="btn btn-neutral float-right" title="高性能MySQL-第三版">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
      <p>MIT License</p>
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
    
      <span><a href="../../database/%E9%AB%98%E6%80%A7%E8%83%BDMySQL/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
